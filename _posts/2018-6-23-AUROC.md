---
title: " Area Under Receiver Operating Characteristic curve (AUROC)"
date: 2018-06-23
tags: [machine learning, data science, AUROC, AUC, Area Under Receiver Operating Characteristic curve ]
header:
  image: "/images/perceptron/percept.jpg"
excerpt: "Machine Learning, Perceptron, Data Science"
mathjax: "true"
---

## What is it?
**AUROC** (or implicitly shorten by **AUC**) is a metric to evaluate and compare the classification performance of machine learning models.
## How is it defined? Why not just use accuracy?
 Accuracy is commonly used, but somtimes it is not enough to reflect the efficiency of the models. Let review some basic definitions of binary-class prediction:
- **_True_ Positive (TP)**: when a sample is Positive, you predict it as **Positive**. (**_Correct_**)
- **_False_ Negative (FN)**: when a sample is Positive, you predict it as **Negative**. (**_Incorrect_**)
- **_False_ Positive (FP)**: when a sample is Negative, you predict it as **Positive**. (**_Incorrect_**)
- **_True_ Negative (TN)**:  when a sample is Negative, you predict it as **Negative**. (**_Correct_**)

In short, **TP, FN, FP, TN** is defined by **\[Correct (_True_)/Incorrect (_False_) \]\[Prediction Label\]**.

- **Accuracy (Acc)**:
 $$Acc = \frac{\text{Number of Correct Prediction}}{\text{Number of total samples}} = \frac{TP + TN}{TP+TN +FP +FN} $$
- **True Positive Rate ( or Sensitivity)(TPR)** :
 $$TPR = \frac{\text{Number of Correct Predicted Positive}}{\text{Number of total Positive samples}} = \frac{TP}{TP+ FP}$$
- **False Positive Rate ( or Fall-out)(FPR)** :
 $$TPR = \frac{\text{Number of InCorrect Predicted Positive}}{\text{Number of total Negative samples}} = \frac{FP}{TN+ FN}$$

Usually, the predictor, e.g. linear regression, outputs the probability of a sample belong to the Postive class, e.g. $$P(x \in \text{Positive}) = p$$. And we predict a sample as Positive if $$p \geq T$$, else Negative.

Hence, by varying the threshold $$T$$, we affect the **TFR,TPR** and **Acc**:
- Specifically, increasing $$T$$ makes predictor more concervative, hence $FPR$ reduces (which is good), but $TPR$ also reduces (which is bad).
- In contrast, reducing $$T$$ makes predictor more agressive, hence $FPR$ increases (which is bad), but $TPR$ also increases (which is good).
- Changing of $$Acc$$ is not monotonic as **TFR** and **TPR** when changing the threshold.

So, if we plot the **TFR** on x-axis, and **TPR** on y-axisFor each threshold value $$T$$, we get the **Receiver Operating Characteristic (ROC)** curve. Hence, we can merge the two metrics TFR and TPR into one metric $$AUROC$ that is independent to the threshold, by computing the Area Under this Curve (**AUC**).
### How to compute it?



Here's some basic text.

And here's some *italics*

Here's some **bold** text.

What about a [link](https://github.com/dataoptimal)?

Here's a bulleted list:
* First item
+ Second item
- Third item

Here's a numbered list:
1. First
2. Second
3. Third

Python code block:
```python
    import numpy as np

    def test_function(x, y):
      z = np.sum(x,y)
      return z
```

R code block:
```r
library(tidyverse)
df <- read_csv("some_file.csv")
head(df)
```

Here's some inline code `x+y`.

Here's an image:
<img src="{{ site.url }}{{ site.baseurl }}/images/perceptron/linsep.jpg" alt="linearly separable data">

Here's another image using Kramdown:
![alt]({{ site.url }}{{ site.baseurl }}/images/perceptron/linsep.jpg)

Here's some math:

$$z=x+y$$

You can also put it inline $$z=x+y$$
