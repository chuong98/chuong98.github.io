---
title: " Area Under Receiver Operating Characteristic curve (AUROC)"
date: 2018-06-23
tags: [machine learning, data science, AUROC, AUC, Area Under Receiver Operating Characteristic curve ]
excerpt: "Machine Learning, Perceptron, Data Science"
mathjax: "true"
---

## What is it?
**AUROC** (or implicitly shorten by **AUC**) is a metric to evaluate and compare the classification performance of machine learning models.
## Why not just use accuracy? Let review basic definitions
 Accuracy is commonly used, but somtimes it is not enough to reflect the efficiency of the models. Let review some basic definitions of binary-class prediction:
- **_True_ Positive (TP)**: when you predict a Positive sample as **Positive**. (**_Correct_**)
- **_False_ Negative (FN)**: when you predict a Positive sample as **Negative**. (**_Incorrect_**)
- **_False_ Positive (FP)**: when you predict a Negative sample as **Positive**. (**_Incorrect_**)
- **_True_ Negative (TN)**:  when you predict a Negative sample as **Negative**. (**_Correct_**)

In short, **TP, FN, FP, TN** is defined by **\[Correct (_True_)/Incorrect (_False_) \]\[Prediction Label\]**.

- **Accuracy (Acc)**:

 $$Acc = \frac{\text{Number of Correct Prediction}}{\text{Number of total samples}} = \frac{TP + TN}{TP+TN +FP +FN} $$

- **True Positive Rate ( or Sensitivity)(TPR)** :

 $$TPR = \frac{\text{Number of Correct Predicted Positive}}{\text{Number of total Positive samples}} = \frac{TP}{TP+ FP}$$

- **False Positive Rate ( or Fall-out)(FPR)** :

 $$FPR = \frac{\text{Number of Incorrect Predicted Positive}}{\text{Number of total Negative samples}} = \frac{FP}{TN+ FN}$$

Usually, the predictor, e.g. linear regression, outputs the probability of a sample belong to the Postive class, e.g. $$P(x \in \text{Positive}) = p$$. And we predict a sample as Positive if $$p \geq T$$, else Negative. Hence, by varying the threshold $$T$$, we affect the **TFR,TPR** and **Acc**:
- Specifically, increasing $$T$$ makes predictor more concervative, hence **FPR** reduces (which is good), but **TPR** also reduces (which is bad).
- In contrast, reducing $$T$$ makes predictor more agressive, hence **FPR** increases (which is bad), but **TPR** also increases (which is good).
- Changing of $$Acc$$ is not monotonic as **TFR** and **TPR** when changing the threshold.
- When choosing the threshold, we compromise between Sensitivity and Fall-Out.

## How is it defined?
So, if for each threshold value $$T$$, we plot a point (**TFR** on x-axis, **TPR** on y-axis), we get the **Receiver Operating Characteristic (ROC)** curve. Hence, by computing the Area Under this Curve (**AUC**), we can merge the two metrics **TFR** and **TPR** into one metric **AUROC**, that is independent to the threshold, as illustrated in the Fig. 1.

![AUROC]({{ site.url }}{{ site.baseurl }}/images/AUROC/AUROC.gif)*Figure 1. Example of AUROC*

## How to compute it?
Let say you apply the prediction to all test samples, here are the steps:
1. Rank the samples by their score $$p$$ from highest to lowest. The middle column $$C$$ contains the true label of the sample.
2. Moving from top to the bottom of the table, and from (0,0) of the plot. At each sample, choose a threshold equal to its value $$p$$
  - If $$x$$ is Postive, we add 1 more Possive sample. That means, the TPR increases by $$\delta y= \frac{1}{\text{Number of Positive samples}}$$. On the plot, move up by $$\delta y$$
  - If $$x$$ is Negative, we add 1 more Negative sample. That means, the TPR increases by $$\delta x= \frac{1}{\text{Number of Negative samples}}$$. On the plot, move right by $$\delta x$$

And then we compute the Area Under this curve.

In python, this is much easier:

```python

  from sklearn.metrics import roc_auc_score

  # y_true is a True label of the sample (Binary class)
  y_predict = Predict_funct(x) # y_predict is the probability (score) predicted
  auroc = roc_auc_score(y_true, y_predict)
```
